{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14793795,"sourceType":"datasetVersion","datasetId":9458313}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAvRgcIku88S","outputId":"9ff30fdb-1bf0-4db2-a857-098790a466ad","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:22:50.085908Z","iopub.execute_input":"2026-02-10T09:22:50.086766Z","iopub.status.idle":"2026-02-10T09:22:50.310461Z","shell.execute_reply.started":"2026-02-10T09:22:50.086715Z","shell.execute_reply":"2026-02-10T09:22:50.309430Z"}},"outputs":[{"name":"stdout","text":"Tue Feb 10 09:22:50 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0             31W /  250W |    1731MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import os\nHOME = os.getcwd()\nprint(f\"HOME: {HOME}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hMJeM5bvNKe","outputId":"4c3682b6-0020-4e63-ff26-d0e8c4f319fe","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:22:50.312512Z","iopub.execute_input":"2026-02-10T09:22:50.313155Z","iopub.status.idle":"2026-02-10T09:22:50.317798Z","shell.execute_reply.started":"2026-02-10T09:22:50.313115Z","shell.execute_reply":"2026-02-10T09:22:50.317012Z"}},"outputs":[{"name":"stdout","text":"HOME: /kaggle/working/yolov9\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"if not os.path.exists(f'{HOME}/yolov9'):\n    !git clone https://github.com/WongKinYiu/yolov9.git\n    print(\"‚úÖ YOLOv9 cloned\")\nelse:\n    print(\"‚úÖ YOLOv9 already exists\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1SqOobvvREG","outputId":"990d1cdd-e516-479a-ac69-ca22f08bab5b","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:22:50.318800Z","iopub.execute_input":"2026-02-10T09:22:50.319234Z","iopub.status.idle":"2026-02-10T09:22:51.356788Z","shell.execute_reply.started":"2026-02-10T09:22:50.319210Z","shell.execute_reply":"2026-02-10T09:22:51.356005Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'yolov9'...\nremote: Enumerating objects: 781, done.\u001b[K\nremote: Total 781 (delta 0), reused 0 (delta 0), pack-reused 781 (from 1)\u001b[K\nReceiving objects: 100% (781/781), 3.27 MiB | 9.30 MiB/s, done.\nResolving deltas: 100% (330/330), done.\n‚úÖ YOLOv9 cloned\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"print(\"üì¶ Installing dependencies...\")\n\n!pip install -q ultralytics\n!pip install -q boxmot  # ByteTrack for tracking\n!pip install -q easyocr\n!pip install -q onnxruntime-gpu\n!pip install -q roboflow\n!pip install -q opencv-python-headless\n!pip uninstall -y ultralytics\n!pip install ultralytics==8.0.196\nprint(\"‚úÖ All dependencies installed!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO-BYvjtvTb9","outputId":"b499b290-8f27-44c9-ceb5-ec5f03b3fa16","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:22:51.358891Z","iopub.execute_input":"2026-02-10T09:22:51.359159Z","iopub.status.idle":"2026-02-10T09:23:16.480339Z","shell.execute_reply.started":"2026-02-10T09:22:51.359124Z","shell.execute_reply":"2026-02-10T09:23:16.479473Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing dependencies...\nFound existing installation: ultralytics 8.0.196\nUninstalling ultralytics-8.0.196:\n  Successfully uninstalled ultralytics-8.0.196\nCollecting ultralytics==8.0.196\n  Using cached ultralytics-8.0.196-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (3.10.0)\nRequirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (2.0.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (4.12.0.88)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (11.3.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (6.0.3)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (2.32.5)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (2.8.0+cu126)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (0.23.0+cu126)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (4.67.1)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (0.13.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (9.0.0)\nRequirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.0.196) (0.1.1.post2209072238)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (26.0rc2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (3.2.5)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.0.196) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.0.196) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (2026.1.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.0.196) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.0.196) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.0.196) (3.0.3)\nUsing cached ultralytics-8.0.196-py3-none-any.whl (631 kB)\nInstalling collected packages: ultralytics\nSuccessfully installed ultralytics-8.0.196\n‚úÖ All dependencies installed!\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"!mkdir -p {HOME}/weights\n\n!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt\n!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n!wget -P {HOME}/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt\n\n!ls -la {HOME}/weights\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dR8gaVnXvYB8","outputId":"2e257a49-a798-4974-a7b5-77299a5e96d0","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:16.482171Z","iopub.execute_input":"2026-02-10T09:23:16.482513Z","iopub.status.idle":"2026-02-10T09:23:22.912658Z","shell.execute_reply.started":"2026-02-10T09:23:16.482474Z","shell.execute_reply":"2026-02-10T09:23:22.911944Z"}},"outputs":[{"name":"stdout","text":"total 402440\ndrwxr-xr-x  2 root root      4096 Feb 10 09:23 .\ndrwxr-xr-x 17 root root      4096 Feb 10 09:23 ..\n-rw-r--r--  1 root root  51508261 Feb 18  2024 gelan-c.pt\n-rw-r--r--  1 root root 117203713 Feb 18  2024 gelan-e.pt\n-rw-r--r--  1 root root 103153312 Feb 18  2024 yolov9-c.pt\n-rw-r--r--  1 root root 140217688 Feb 18  2024 yolov9-e.pt\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"%cd {HOME}/yolov9\n\nimport roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\n\nproject = rf.workspace(\"arvind-kumar-wjygd\").project(\"anpr2-syxl7\")\nversion = project.version(8)\ndataset = version.download(\"yolov9\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRtEQWYzv1b4","outputId":"7719747f-a083-4979-84f0-86b06c5ebcc6","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:22.913980Z","iopub.execute_input":"2026-02-10T09:23:22.914289Z","iopub.status.idle":"2026-02-10T09:23:25.703181Z","shell.execute_reply.started":"2026-02-10T09:23:22.914260Z","shell.execute_reply":"2026-02-10T09:23:25.702511Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov9/yolov9\nYou are already logged into Roboflow. To make a different login,run roboflow.login(force=True).\nloading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in ANPR2-8 to yolov9:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15578/15578 [00:00<00:00, 28649.88it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to ANPR2-8 in yolov9:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 626/626 [00:00<00:00, 8753.95it/s]\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import yaml\n\ndata_yaml = {\n    'train': f'{HOME}/yolov9/ANPR2-8/train/images',\n    'val': f'{HOME}/yolov9/ANPR2-8/valid/images',\n    'nc': 1,\n    'names': ['license_plate']\n}\n\nwith open(f'{HOME}/yolov9/ANPR2-8/data.yaml', 'w') as f:\n    yaml.dump(data_yaml, f)\n\nprint(\"‚úÖ data.yaml created!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RFBnsAWv7lh","outputId":"5c3e1346-5ed7-47b8-a3e0-6ee48c4f04c7","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:25.704249Z","iopub.execute_input":"2026-02-10T09:23:25.704483Z","iopub.status.idle":"2026-02-10T09:23:25.710079Z","shell.execute_reply.started":"2026-02-10T09:23:25.704461Z","shell.execute_reply":"2026-02-10T09:23:25.709330Z"}},"outputs":[{"name":"stdout","text":"‚úÖ data.yaml created!\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import re\n\ndef patch_torch_load(file_path):\n    \"\"\"Patch torch.load for PyTorch 2.6+ compatibility\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            return False\n\n        with open(file_path, 'r') as f:\n            content = f.read()\n\n        if 'torch.load' in content and 'weights_only=False' not in content:\n            # Add weights_only=False to all torch.load calls\n            content = re.sub(\n                r'torch\\.load\\(([^)]+)\\)',\n                r'torch.load(\\1, weights_only=False)',\n                content\n            )\n\n            with open(file_path, 'w') as f:\n                f.write(content)\n\n            print(f\"‚úÖ Patched: {file_path}\")\n            return True\n        else:\n            print(f\"‚è≠Ô∏è  No changes needed: {file_path}\")\n            return False\n    except Exception as e:\n        print(f\"‚ùå Error patching {file_path}: {e}\")\n        return False\n\n# Files to patch\nfiles_to_patch = [\n    f'{HOME}/yolov9/train.py',\n    f'{HOME}/yolov9/val.py',\n    f'{HOME}/yolov9/detect.py',\n    f'{HOME}/yolov9/utils/general.py',\n    f'{HOME}/yolov9/utils/torch_utils.py',\n    f'{HOME}/yolov9/models/experimental.py',\n]\n\nprint(\"üîß Patching YOLOv9 files...\\n\")\nfor file in files_to_patch:\n    patch_torch_load(file)\n\nprint(\"\\n‚úÖ Patching complete!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEtxWsMZwQHg","outputId":"3195844c-849a-485f-9116-55663cfbe0bb","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:25.711062Z","iopub.execute_input":"2026-02-10T09:23:25.711307Z","iopub.status.idle":"2026-02-10T09:23:25.725973Z","shell.execute_reply.started":"2026-02-10T09:23:25.711285Z","shell.execute_reply":"2026-02-10T09:23:25.725275Z"}},"outputs":[{"name":"stdout","text":"üîß Patching YOLOv9 files...\n\n‚úÖ Patched: /kaggle/working/yolov9/yolov9/train.py\n‚è≠Ô∏è  No changes needed: /kaggle/working/yolov9/yolov9/val.py\n‚è≠Ô∏è  No changes needed: /kaggle/working/yolov9/yolov9/detect.py\n‚úÖ Patched: /kaggle/working/yolov9/yolov9/utils/general.py\n‚è≠Ô∏è  No changes needed: /kaggle/working/yolov9/yolov9/utils/torch_utils.py\n‚úÖ Patched: /kaggle/working/yolov9/yolov9/models/experimental.py\n\n‚úÖ Patching complete!\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ================================================================\n# CELL 6.5: Fix PyTorch 2.6 Model Loading\n# ================================================================\nprint(\"üîß Patching PyTorch 2.6 compatibility...\")\n\nimport torch\nimport numpy as np\n\n# Add safe globals for PyTorch 2.6\ntorch.serialization.add_safe_globals([\n    np.core.multiarray._reconstruct,\n    np.ndarray,\n    np.dtype,\n    np.core.multiarray.scalar,\n    np.core._multiarray_umath._reconstruct\n])\n\nprint(\"‚úÖ PyTorch 2.6 compatibility fixed!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5V0qrBLWDlI","outputId":"76b2af14-4fbe-4689-c88b-c68783ce935c","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:25.726939Z","iopub.execute_input":"2026-02-10T09:23:25.727277Z","iopub.status.idle":"2026-02-10T09:23:25.738580Z","shell.execute_reply.started":"2026-02-10T09:23:25.727243Z","shell.execute_reply":"2026-02-10T09:23:25.738023Z"}},"outputs":[{"name":"stdout","text":"üîß Patching PyTorch 2.6 compatibility...\n‚úÖ PyTorch 2.6 compatibility fixed!\n","output_type":"stream"},{"name":"stderr","text":"numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"%cd /kaggle/working/yolov9\n\n!python train.py \\\n--batch 16 --epochs 25 --img 640 --device 0 --min-items 0 --close-mosaic 15 \\\n--data /kaggle/working/yolov9/ANPR2-8/data.yaml \\\n--weights /kaggle/working/weights/gelan-c.pt \\\n--cfg models/detect/gelan-c.yaml \\\n--hyp hyp.scratch-high.yaml\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oy6EjFaEwfh8","outputId":"bd226465-1387-4d0e-f8ca-ee3d8359f4d0","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:23:25.741187Z","iopub.execute_input":"2026-02-10T09:23:25.741433Z","iopub.status.idle":"2026-02-10T09:31:49.497666Z","shell.execute_reply.started":"2026-02-10T09:23:25.741410Z","shell.execute_reply":"2026-02-10T09:31:49.496643Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov9\n2026-02-10 09:23:31.422870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770715411.444058    1235 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770715411.450984    1235 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770715411.469024    1235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770715411.469060    1235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770715411.469065    1235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770715411.469071    1235 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=/kaggle/working/weights/gelan-c.pt, cfg=models/detect/gelan-c.yaml, data=/kaggle/working/yolov9/ANPR2-8/data.yaml, hyp=hyp.scratch-high.yaml, epochs=25, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\nYOLO üöÄ v0.1-104-g5b1ea9a Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO üöÄ in ClearML\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO üöÄ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nOverriding model.yaml nc=80 with nc=1\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n 22      [15, 18, 21]  1   5491411  models.yolo.DDetect                     [1, [256, 512, 512]]          \ngelan-c summary: 621 layers, 25437843 parameters, 25437827 gradients, 103.2 GFLOPs\n\nTransferred 931/937 items from /kaggle/working/weights/gelan-c.pt\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias\n\u001b[34m\u001b[1malbumentations: \u001b[0m1 validation error for InitSchema\nsize\n  Field required [type=missing, input_value={'scale': (0.8, 1.0), 'ra...: None, 'strict': False}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/yolov9/ANPR2-8/train/labels.cache... 278 images,\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/yolov9/ANPR2-8/valid/labels.cache... 6 images, 0 b\u001b[0m\nPlotting labels to runs/train/exp3/labels.jpg... \n/kaggle/working/yolov9/train.py:244: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=amp)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/train/exp3\u001b[0m\nStarting training for 25 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  0%|          | 0/18 00:00/kaggle/working/yolov9/train.py:302: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/24      11.7G      1.734      4.566      1.638         29        640:  Exception in thread Thread-5 (plot_images):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 300, in plot_images\n    annotator.box_label(box, label, color=color)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 86, in box_label\n    w, h = self.font.getsize(label)  # text width, height\n           ^^^^^^^^^^^^^^^^^\nAttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n       0/24      11.7G      1.734      4.566      1.638         29        640:  /kaggle/working/yolov9/train.py:302: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/24      11.7G      1.752      4.656       1.63         36        640:  Exception in thread Thread-6 (plot_images):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 300, in plot_images\n    annotator.box_label(box, label, color=color)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 86, in box_label\n    w, h = self.font.getsize(label)  # text width, height\n           ^^^^^^^^^^^^^^^^^\nAttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n       0/24      11.7G      1.782       4.78      1.652         25        640:  Exception in thread Thread-7 (plot_images):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 300, in plot_images\n    annotator.box_label(box, label, color=color)\n  File \"/kaggle/working/yolov9/utils/plots.py\", line 86, in box_label\n    w, h = self.font.getsize(label)  # text width, height\n           ^^^^^^^^^^^^^^^^^\nAttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n       0/24      11.7G      1.729      3.805      1.614         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.933      0.696      0.797      0.595\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/24      11.8G      1.552      1.628      1.373         14        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.861      0.928      0.934      0.728\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/24      11.9G      1.481      1.138       1.32         23        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.898       0.95      0.931       0.65\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/24      11.9G      1.472      1.258      1.307         14        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.869      0.998      0.951      0.642\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/24      11.9G      1.447      1.222      1.239          9        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.924       0.61       0.92      0.637\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/24      11.9G      1.468      1.123      1.276         10        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.711        0.4      0.435      0.324\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/24        12G      1.516      1.112      1.297         16        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.593        0.8      0.784      0.477\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/24        12G      1.471      1.029      1.304         10        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.713       0.75      0.808      0.425\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/24      12.1G       1.52      1.041      1.318         18        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.833        0.6      0.791      0.483\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/24      12.1G      1.456      1.039      1.252         26        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.668      0.904      0.839       0.44\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/24      12.1G      1.558     0.9837      1.432          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20       0.56        0.7      0.593      0.322\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/24      12.1G      1.486      1.018      1.364          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.809        0.9      0.904      0.576\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/24      12.1G      1.469     0.9526      1.352          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.799          1      0.905      0.516\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/24      12.1G      1.446     0.9358      1.367          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.676        0.9      0.831      0.541\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/24      12.1G      1.449     0.8905      1.329          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.944      0.845      0.918       0.55\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/24      12.1G      1.565      1.006      1.406          5        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.813       0.87      0.908      0.591\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/24      12.1G      1.413     0.8715      1.316          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.747          1      0.913      0.515\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/24      12.1G      1.418     0.8509      1.344          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.733       0.85      0.861      0.603\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/24      12.1G      1.403     0.7914      1.307         10        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.867        0.9      0.892        0.6\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/24      12.1G      1.313     0.7333      1.263          7        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.797       0.85      0.886      0.556\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/24      12.1G      1.334     0.7804        1.3          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.799      0.794      0.831       0.57\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/24      12.1G      1.281     0.7637      1.263          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.925       0.85      0.936       0.66\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/24      12.1G      1.294       0.73      1.259          7        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20       0.93       0.95      0.975       0.67\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/24      12.1G      1.283     0.7092      1.275          7        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.987       0.85      0.967      0.683\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/24      12.1G      1.197     0.6591      1.233          6        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all          6         20      0.941       0.85      0.967      0.682\n\n25 epochs completed in 0.127 hours.\nTraceback (most recent call last):\n  File \"/kaggle/working/yolov9/train.py\", line 634, in <module>\n    main(opt)\n  File \"/kaggle/working/yolov9/train.py\", line 528, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/kaggle/working/yolov9/train.py\", line 403, in train\n    strip_optimizer(f, last_striped)  # strip optimizers\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/utils/general.py\", line 999, in strip_optimizer\n    x = torch.load(f, map_location=torch.device('cpu', weights_only=False))\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: device() got an unexpected keyword argument 'weights_only'\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!ls /kaggle/working/yolov9/runs/train/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-0Ijzz_2bOc","outputId":"07a03e19-4f55-4bb9-d6cf-be2c4e141c10","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:31:49.499149Z","iopub.execute_input":"2026-02-10T09:31:49.499454Z","iopub.status.idle":"2026-02-10T09:31:49.680875Z","shell.execute_reply.started":"2026-02-10T09:31:49.499421Z","shell.execute_reply":"2026-02-10T09:31:49.680078Z"}},"outputs":[{"name":"stdout","text":"exp  exp2  exp3\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import glob\nexp_folders = sorted(glob.glob(f'/kaggle/working/yolov9/runs/train/exp*'))\nlatest_exp = exp_folders[-1] if exp_folders else None\n\nif latest_exp:\n    print(f\"üìÅ Latest training: {latest_exp}\")\n\n    from IPython.display import Image, display\n\n    # Display confusion matrix\n    conf_matrix = f\"{latest_exp}/confusion_matrix.png\"\n    if os.path.exists(conf_matrix):\n        print(\"\\nüìä Confusion Matrix:\")\n        display(Image(filename=conf_matrix, width=800))\n\n    # Display results\n    results_img = f\"{latest_exp}/results.png\"\n    if os.path.exists(results_img):\n        print(\"\\nüìà Training Results:\")\n        display(Image(filename=results_img, width=1000))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eu1dKbub2l2n","outputId":"3d56d239-84ea-4693-a928-e92f19a7d03f","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:11.309708Z","iopub.execute_input":"2026-02-10T09:36:11.310071Z","iopub.status.idle":"2026-02-10T09:36:11.316912Z","shell.execute_reply.started":"2026-02-10T09:36:11.310042Z","shell.execute_reply":"2026-02-10T09:36:11.316256Z"}},"outputs":[{"name":"stdout","text":"üìÅ Latest training: /kaggle/working/yolov9/runs/train/exp3\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"%cd /kaggle/working/yolov9\n\n# Use the best weights from latest training\nBEST_WEIGHTS = f\"/kaggle/working/yolov9/runs/train/exp2/weights/best.pt\"\n\n!python val.py \\\n--img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 \\\n--data /kaggle/working/yolov9/ANPR2-8/data.yaml \\\n--weights {BEST_WEIGHTS}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NnZtvv-2pMF","outputId":"db557160-0a98-463a-9467-a439b856ab57","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:14.158578Z","iopub.execute_input":"2026-02-10T09:36:14.159127Z","iopub.status.idle":"2026-02-10T09:36:20.138596Z","shell.execute_reply.started":"2026-02-10T09:36:14.159090Z","shell.execute_reply":"2026-02-10T09:36:20.137871Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov9\n\u001b[34m\u001b[1mval: \u001b[0mdata=/kaggle/working/yolov9/ANPR2-8/data.yaml, weights=['/kaggle/working/yolov9/runs/train/exp2/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.7, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False, min_items=0\nYOLO üöÄ v0.1-104-g5b1ea9a Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\nTraceback (most recent call last):\n  File \"/kaggle/working/yolov9/val.py\", line 389, in <module>\n    main(opt)\n  File \"/kaggle/working/yolov9/val.py\", line 362, in main\n    run(**vars(opt))\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/val.py\", line 122, in run\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/models/common.py\", line 705, in __init__\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/models/experimental.py\", line 243, in attempt_load\n    ckpt = torch.load(attempt_download(w, weights_only=False), map_location='cpu')  # load\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: attempt_download() got an unexpected keyword argument 'weights_only'\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"%cd {HOME}/yolov9\n\n!python detect.py \\\n--img 1280 --conf 0.1 --device 0 \\\n--weights {BEST_WEIGHTS} \\\n--source /kaggle/working/yolov9/ANPR2-8/test/images\n\n# Display results\nimport glob\nfrom IPython.display import Image, display\n\nprint(\"üñºÔ∏è Detection Results:\")\nfor image_path in glob.glob(f'{HOME}/yolov9/runs/detect/exp*/*.jpg')[:5]:\n    display(Image(filename=image_path, width=600))\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6N2a_yn42vKJ","outputId":"0c062990-7ae0-4a4b-a293-e8d87635dd65","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:20.140266Z","iopub.execute_input":"2026-02-10T09:36:20.140538Z","iopub.status.idle":"2026-02-10T09:36:26.020675Z","shell.execute_reply.started":"2026-02-10T09:36:20.140511Z","shell.execute_reply":"2026-02-10T09:36:26.019713Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov9/yolov9\n\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/kaggle/working/yolov9/runs/train/exp2/weights/best.pt'], source=/kaggle/working/yolov9/ANPR2-8/test/images, data=data/coco128.yaml, imgsz=[1280, 1280], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\nYOLO üöÄ v0.1-104-g5b1ea9a Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\nTraceback (most recent call last):\n  File \"/kaggle/working/yolov9/yolov9/detect.py\", line 231, in <module>\n    main(opt)\n  File \"/kaggle/working/yolov9/yolov9/detect.py\", line 226, in main\n    run(**vars(opt))\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/yolov9/detect.py\", line 68, in run\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/yolov9/models/common.py\", line 705, in __init__\n    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/yolov9/models/experimental.py\", line 243, in attempt_load\n    ckpt = torch.load(attempt_download(w, weights_only=False), map_location='cpu')  # load\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: attempt_download() got an unexpected keyword argument 'weights_only'\nüñºÔ∏è Detection Results:\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"ANPR_SCRIPT = '''\nimport argparse\nimport cv2\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom ultralytics import YOLO\nfrom boxmot import ByteTrack as BYTETracker\nimport easyocr\nimport time\nimport json\nfrom collections import Counter\n\nclass ANPRWithTracking:\n    def __init__(self, weights, conf_thres=0.25, iou_thres=0.45):\n        \"\"\"ANPR system with ByteTrack tracking\"\"\"\n        print(\"üöÄ Initializing ANPR with ByteTrack...\")\n\n        # Load YOLO model\n        self.model = YOLO(weights)\n        self.conf_thres = conf_thres\n        self.iou_thres = iou_thres\n\n        # Initialize ByteTrack\n        self.tracker = BYTETracker(\n            track_thresh=0.5,\n            track_buffer=30,\n            match_thresh=0.8,\n            frame_rate=30\n        )\n        print(\"‚úÖ ByteTrack initialized\")\n\n        # Initialize EasyOCR\n        print(\"üöÄ Loading OCR model...\")\n        self.reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available(), verbose=False)\n        print(\"‚úÖ OCR model loaded\")\n\n        # Storage\n        self.tracked_plates = {}\n        self.plate_history = {}\n        self.frame_count = 0\n\n    def detect_plates(self, frame):\n        \"\"\"Detect license plates\"\"\"\n        results = self.model(frame, conf=self.conf_thres, iou=self.iou_thres, verbose=False)[0]\n\n        detections = []\n        for box in results.boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            conf = float(box.conf[0].cpu().numpy())\n            cls = int(box.cls[0].cpu().numpy())\n            detections.append([x1, y1, x2, y2, conf, cls])\n\n        return np.array(detections) if detections else np.empty((0, 6))\n\n    def extract_plate_text(self, frame, bbox):\n        \"\"\"Extract text from license plate\"\"\"\n        x1, y1, x2, y2 = map(int, bbox[:4])\n\n        # Add padding\n        pad = 5\n        h, w = frame.shape[:2]\n        x1 = max(0, x1 - pad)\n        y1 = max(0, y1 - pad)\n        x2 = min(w, x2 + pad)\n        y2 = min(h, y2 + pad)\n\n        plate_img = frame[y1:y2, x1:x2]\n\n        if plate_img.size == 0:\n            return None, 0.0\n\n        # Preprocess\n        gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n        gray = cv2.bilateralFilter(gray, 11, 17, 17)\n\n        try:\n            results = self.reader.readtext(gray, detail=1)\n            if results:\n                texts = [text for (_, text, _) in results]\n                confs = [conf for (_, _, conf) in results]\n\n                plate_text = ''.join(texts).replace(' ', '').upper()\n                avg_conf = np.mean(confs)\n\n                if len(plate_text) >= 4:\n                    return plate_text, avg_conf\n        except:\n            pass\n\n        return None, 0.0\n\n    def get_best_plate_text(self, track_id):\n        \"\"\"Get most common plate reading\"\"\"\n        if track_id not in self.plate_history or not self.plate_history[track_id]:\n            return None\n\n        readings = self.plate_history[track_id]\n        most_common = Counter(readings).most_common(1)[0][0]\n        return most_common\n\n    def process_video(self, source, output=None, save_txt=False):\n        \"\"\"Process video with tracking\"\"\"\n        cap = cv2.VideoCapture(source)\n\n        if not cap.isOpened():\n            print(f\"‚ùå Cannot open video: {source}\")\n            return\n\n        # Video properties\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        print(f\"\\\\nüìπ Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n\n        # Video writer\n        if output:\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(output, fourcc, fps, (width, height))\n        else:\n            out = None\n\n        start_time = time.time()\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Detect plates\n            detections = self.detect_plates(frame)\n\n            # Track\n            if len(detections) > 0:\n                tracks = self.tracker.update(detections, frame)\n            else:\n                tracks = np.empty((0, 8))\n\n            # Process tracks\n            for track in tracks:\n                if len(track) < 5:\n                    continue\n\n                x1, y1, x2, y2, track_id = track[:5]\n                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n                track_id = int(track_id)\n\n                # OCR every 10 frames\n                if self.frame_count % 10 == 0:\n                    plate_text, conf = self.extract_plate_text(frame, track)\n                    if plate_text and conf > 0.5:\n                        if track_id not in self.plate_history:\n                            self.plate_history[track_id] = []\n                        self.plate_history[track_id].append(plate_text)\n\n                # Update best plate\n                if track_id in self.plate_history:\n                    best_text = self.get_best_plate_text(track_id)\n                    if best_text:\n                        self.tracked_plates[track_id] = best_text\n\n                # Draw\n                color = (0, 255, 0) if track_id in self.tracked_plates else (255, 165, 0)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n                label = f\"ID:{track_id}\"\n                if track_id in self.tracked_plates:\n                    label += f\" | {self.tracked_plates[track_id]}\"\n\n                (w_txt, h_txt), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n                cv2.rectangle(frame, (x1, y1-h_txt-10), (x1+w_txt+5, y1), color, -1)\n                cv2.putText(frame, label, (x1+2, y1-5),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n\n            # Stats\n            cv2.putText(frame, f\"Frame: {self.frame_count}/{total_frames} | Plates: {len(self.tracked_plates)}\",\n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n\n            if out:\n                out.write(frame)\n\n            self.frame_count += 1\n\n            if self.frame_count % 30 == 0:\n                print(f\"Progress: {self.frame_count}/{total_frames} | Tracked: {len(self.tracked_plates)}\")\n\n        cap.release()\n        if out:\n            out.release()\n\n        elapsed = time.time() - start_time\n        print(f\"\\\\n‚úÖ Complete! Time: {elapsed:.2f}s | FPS: {self.frame_count/elapsed:.1f}\")\n        print(f\"üöó Tracked plates: {len(self.tracked_plates)}\")\n\n        for track_id, plate in sorted(self.tracked_plates.items()):\n            print(f\"   ID {track_id}: {plate}\")\n\n        # Save results\n        if save_txt:\n            results = {\n                'tracked_plates': self.tracked_plates,\n                'total_frames': self.frame_count,\n                'processing_time': elapsed\n            }\n            with open('tracking_results.json', 'w') as f:\n                json.dump(results, f, indent=2)\n            print(\"\\\\nüíæ Results saved to tracking_results.json\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', type=str, required=True, help='model path')\n    parser.add_argument('--source', type=str, required=True, help='video path')\n    parser.add_argument('--output', type=str, default='output_tracked.mp4', help='output video path')\n    parser.add_argument('--conf', type=float, default=0.25, help='confidence threshold')\n    parser.add_argument('--iou', type=float, default=0.45, help='IOU threshold')\n    parser.add_argument('--save-txt', action='store_true', help='save results to txt')\n\n    args = parser.parse_args()\n\n    anpr = ANPRWithTracking(\n        weights=args.weights,\n        conf_thres=args.conf,\n        iou_thres=args.iou\n    )\n\n    anpr.process_video(\n        source=args.source,\n        output=args.output,\n        save_txt=args.save_txt\n    )\n'''\n\n# Save the script\nwith open(f'/kaggle/working/yolov9/anpr_tracking.py', 'w') as f:\n    f.write(ANPR_SCRIPT)\n\nprint(\"‚úÖ anpr_tracking.py created with ByteTrack support!\")\n\n# ================================================================\n# CELL 14: Upload Test Video\n# ================================================================\n# Replace the Colab upload cell with:\nimport os\n\n# Video from Kaggle dataset\nvideo_file = '/kaggle/input/anpr-data-set'\n\nif os.path.exists(video_file):\n    print(f\"‚úÖ Video found: {video_file}\")\nelse:\n    print(\"‚ùå Video not found. Please add dataset!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"jeaCzTWZ28y3","outputId":"012d247e-774e-45c9-e628-10c61e3d4012","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:26.022203Z","iopub.execute_input":"2026-02-10T09:36:26.022484Z","iopub.status.idle":"2026-02-10T09:36:26.037528Z","shell.execute_reply.started":"2026-02-10T09:36:26.022456Z","shell.execute_reply":"2026-02-10T09:36:26.036842Z"}},"outputs":[{"name":"stdout","text":"‚úÖ anpr_tracking.py created with ByteTrack support!\n‚úÖ Video found: /kaggle/input/anpr-data-set\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"%cd /kaggle/working/yolov9\n\nif video_file:\n    !python anpr_tracking.py \\\n    --weights {BEST_WEIGHTS} \\\n    --source {video_file} \\\n    --output output_with_tracking.mp4 \\\n    --conf 0.1 \\\n    --save-txt\n\n    print(\"\\n‚úÖ Processing complete!\")\n    print(\"üì∫ Output saved to: output_with_tracking.mp4\")\nelse:\n    print(\"‚ùå No video file available!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"YaWDpXn43Mt9","outputId":"543c6cba-93c4-485b-ea88-7a293240df05","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:32:01.769135Z","iopub.execute_input":"2026-02-10T09:32:01.769362Z","iopub.status.idle":"2026-02-10T09:32:08.233571Z","shell.execute_reply.started":"2026-02-10T09:32:01.769342Z","shell.execute_reply":"2026-02-10T09:32:08.232846Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov9\nüöÄ Initializing ANPR with ByteTrack...\nTraceback (most recent call last):\n  File \"/kaggle/working/yolov9/anpr_tracking.py\", line 222, in <module>\n    anpr = ANPRWithTracking(\n           ^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/yolov9/anpr_tracking.py\", line 20, in __init__\n    self.model = YOLO(weights)\n                 ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 97, in __init__\n    self._load(model, task)\n  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 149, in _load\n    self.model, self.ckpt = attempt_load_one_weight(weights)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\", line 628, in attempt_load_one_weight\n    ckpt, weight = torch_safe_load(weight)  # load ckpt\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\", line 567, in torch_safe_load\n    return torch.load(file, map_location='cpu'), file  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n         ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n    super().__init__(open(name, mode))\n                     ^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/weights/best.pt'\n\n‚úÖ Processing complete!\nüì∫ Output saved to: output_with_tracking.mp4\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"from IPython.display import Video, display\n\nif os.path.exists('output_with_tracking.mp4'):\n    print(\"üì∫ Output Video with Tracking:\")\n    display(Video('output_with_tracking.mp4', width=800))\nelse:\n    print(\"‚ùå Output video not found!\")\n\n# Display tracking results\nif os.path.exists('tracking_results.json'):\n    import json\n    with open('tracking_results.json', 'r') as f:\n        results = json.load(f)\n\n    print(\"\\nüìä Tracking Results:\")\n    print(f\"   Total Frames: {results['total_frames']}\")\n    print(f\"   Processing Time: {results['processing_time']:.2f}s\")\n    print(f\"   Tracked Plates: {len(results['tracked_plates'])}\")\n    print(\"\\nüöó License Plates:\")\n    for track_id, plate in results['tracked_plates'].items():\n        print(f\"   ID {track_id}: {plate}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awa5g-Uq87cD","outputId":"0c1fb1b4-cfc9-4681-f3ef-dbe43dc9e724","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:26.038896Z","iopub.execute_input":"2026-02-10T09:36:26.039105Z","iopub.status.idle":"2026-02-10T09:36:26.051892Z","shell.execute_reply.started":"2026-02-10T09:36:26.039085Z","shell.execute_reply":"2026-02-10T09:36:26.051124Z"}},"outputs":[{"name":"stdout","text":"‚ùå Output video not found!\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"üîÑ ONNX MODEL SETUP\")\nprint(\"=\"*60)\n\nfrom pathlib import Path\nimport os\n\n# ONNX was already created successfully in your earlier run!\nONNX_WEIGHTS = str(Path(BEST_WEIGHTS).with_suffix('.onnx'))\n\nif os.path.exists(ONNX_WEIGHTS):\n    print(f\"‚úÖ ONNX model found: {ONNX_WEIGHTS}\")\n    print(f\"üìä Size: {os.path.getsize(ONNX_WEIGHTS) / 1024 / 1024:.1f} MB\")\nelse:\n    print(f\"‚ùå ONNX model not found at: {ONNX_WEIGHTS}\")\n    print(\"Please check the path or re-export manually\")\n    ONNX_WEIGHTS = None\n\nprint(\"\\n‚úÖ Ready for benchmarking!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"GjXqVGOW9HGa","outputId":"7678ff17-d63e-40d3-eb5f-49ac4e0d29d2","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:28.261222Z","iopub.execute_input":"2026-02-10T09:36:28.261961Z","iopub.status.idle":"2026-02-10T09:36:28.267767Z","shell.execute_reply.started":"2026-02-10T09:36:28.261930Z","shell.execute_reply":"2026-02-10T09:36:28.267017Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüîÑ ONNX MODEL SETUP\n============================================================\n‚úÖ ONNX model found: /kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\nüìä Size: 96.4 MB\n\n‚úÖ Ready for benchmarking!\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"if ONNX_WEIGHTS:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä BENCHMARKING PERFORMANCE\")\n    print(\"=\"*60)\n\n    import time\n    import numpy as np\n    import onnxruntime as ort\n    import torch\n    import matplotlib.pyplot as plt\n\n    # Test image\n    test_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n    num_runs = 50\n\n    # PyTorch benchmark - Load model directly with torch\n    print(\"‚öôÔ∏è  Testing PyTorch...\")\n\n    ckpt = torch.load(BEST_WEIGHTS, map_location='cpu', weights_only=False)\n    pytorch_model = ckpt['model'].float().eval().cuda()\n\n    pytorch_times = []\n\n    # Warmup\n    with torch.no_grad():\n        for _ in range(5):\n            img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().cuda() / 255.0\n            _ = pytorch_model(img_tensor)\n\n    # Benchmark\n    for i in range(num_runs):\n        img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().cuda() / 255.0\n\n        torch.cuda.synchronize()\n        start = time.time()\n\n        with torch.no_grad():\n            _ = pytorch_model(img_tensor)\n\n        torch.cuda.synchronize()\n        pytorch_times.append((time.time() - start) * 1000)\n\n        if (i + 1) % 10 == 0:\n            print(f\"  Progress: {i+1}/{num_runs}\")\n\n    pytorch_avg = np.mean(pytorch_times[5:])\n\n    # ONNX benchmark\n    print(\"\\n‚öôÔ∏è  Testing ONNX...\")\n    ort_session = ort.InferenceSession(\n        ONNX_WEIGHTS,\n        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n    )\n\n    input_name = ort_session.get_inputs()[0].name\n    test_input = test_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n    test_input = np.expand_dims(test_input, axis=0)\n\n    # Warmup\n    for _ in range(5):\n        _ = ort_session.run(None, {input_name: test_input})\n\n    onnx_times = []\n    for i in range(num_runs):\n        start = time.time()\n        _ = ort_session.run(None, {input_name: test_input})\n        onnx_times.append((time.time() - start) * 1000)\n\n        if (i + 1) % 10 == 0:\n            print(f\"  Progress: {i+1}/{num_runs}\")\n\n    onnx_avg = np.mean(onnx_times[5:])\n\n    # Results\n    speedup = pytorch_avg / onnx_avg\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä RESULTS\")\n    print(\"=\"*60)\n    print(f\"PyTorch: {pytorch_avg:.2f}ms | {1000/pytorch_avg:.1f} FPS\")\n    print(f\"ONNX:    {onnx_avg:.2f}ms | {1000/onnx_avg:.1f} FPS\")\n    print(f\"Speedup: {speedup:.2f}x faster ‚ö°\")\n    print(\"=\"*60)\n\n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n    axes[0].bar(['PyTorch', 'ONNX'], [pytorch_avg, onnx_avg], color=['#3498db', '#2ecc71'])\n    axes[0].set_ylabel('Inference Time (ms)')\n    axes[0].set_title('Speed Comparison')\n    axes[0].grid(axis='y', alpha=0.3)\n    for i, v in enumerate([pytorch_avg, onnx_avg]):\n        axes[0].text(i, v + 1, f'{v:.1f}ms', ha='center', fontweight='bold')\n\n    axes[1].bar(['PyTorch', 'ONNX'], [1000/pytorch_avg, 1000/onnx_avg], color=['#3498db', '#2ecc71'])\n    axes[1].set_ylabel('FPS')\n    axes[1].set_title('Frames Per Second')\n    axes[1].grid(axis='y', alpha=0.3)\n    for i, v in enumerate([1000/pytorch_avg, 1000/onnx_avg]):\n        axes[1].text(i, v + 1, f'{v:.1f}', ha='center', fontweight='bold')\n\n    plt.tight_layout()\n    plt.savefig('benchmark_comparison.png', dpi=150)\n    plt.show()\n\n    print(\"\\n‚úÖ Benchmark complete!\")","metadata":{"id":"NRI-HpFN9W4V","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:31.137117Z","iopub.execute_input":"2026-02-10T09:36:31.137411Z","iopub.status.idle":"2026-02-10T09:36:35.019890Z","shell.execute_reply.started":"2026-02-10T09:36:31.137385Z","shell.execute_reply":"2026-02-10T09:36:35.019031Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüìä BENCHMARKING PERFORMANCE\n============================================================\n‚öôÔ∏è  Testing PyTorch...\n  Progress: 10/50\n  Progress: 20/50\n  Progress: 30/50\n  Progress: 40/50\n  Progress: 50/50\n\n‚öôÔ∏è  Testing ONNX...\n  Progress: 10/50\n  Progress: 20/50\n  Progress: 30/50\n  Progress: 40/50\n  Progress: 50/50\n\n============================================================\nüìä RESULTS\n============================================================\nPyTorch: 22.63ms | 44.2 FPS\nONNX:    26.97ms | 37.1 FPS\nSpeedup: 0.84x faster ‚ö°\n============================================================\n\n‚úÖ Benchmark complete!\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"print(\"\\nüì• Download Results:\")\nprint(\"=\"*60)\n\nfiles_to_download = [\n    'output_with_tracking.mp4',\n    'tracking_results.json',\n    BEST_WEIGHTS,\n    ONNX_WEIGHTS,\n    'benchmark_comparison.png'\n]\n\nfor file in files_to_download:\n    if file and os.path.exists(file):\n        print(f\"‚úÖ {file}\")\n        try:\n            files.download(file)\n        except:\n            print(f\"   ‚ö†Ô∏è  Download failed (file may be too large)\")\n    else:\n        print(f\"‚è≠Ô∏è  {file} - not found\")\n\nprint(\"\\n‚úÖ All done!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"A3TP58oE9jj9","outputId":"ff74d7af-cf4f-483f-baa9-fc8955d69bb1","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:37.670449Z","iopub.execute_input":"2026-02-10T09:36:37.670767Z","iopub.status.idle":"2026-02-10T09:36:37.685853Z","shell.execute_reply.started":"2026-02-10T09:36:37.670740Z","shell.execute_reply":"2026-02-10T09:36:37.685094Z"}},"outputs":[{"name":"stdout","text":"\nüì• Download Results:\n============================================================\n‚è≠Ô∏è  output_with_tracking.mp4 - not found\n‚è≠Ô∏è  tracking_results.json - not found\n‚úÖ /kaggle/working/yolov9/runs/train/exp2/weights/best.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_c6339db3-de46-47ae-85db-e6d2f06a29e8\", \"best.pt\", 204620845)"},"metadata":{}},{"name":"stdout","text":"‚úÖ /kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_7c6ae408-93ba-46c8-a161-b1ddd24c9ac3\", \"best.onnx\", 101096639)"},"metadata":{}},{"name":"stdout","text":"‚úÖ benchmark_comparison.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_cc9aa1e9-21b9-4e97-97c0-094c777a5c35\", \"benchmark_comparison.png\", 40574)"},"metadata":{}},{"name":"stdout","text":"\n‚úÖ All done!\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import shutil\nimport os\n\nprint(\"üì• Loading best.onnx from Kaggle dataset...\")\n\n# Path to your uploaded ONNX file in Kaggle dataset\n# Replace 'your-dataset-name' with your actual dataset name\nUPLOADED_ONNX = '/kaggle/input/anpr-data-set/best.onnx'\n\n# Check if file exists\nif os.path.exists(UPLOADED_ONNX):\n    # Create target directory\n    os.makedirs(f\"{HOME}/yolov9/runs/train/exp/weights\", exist_ok=True)\n    \n    # Copy to correct location\n    ONNX_WEIGHTS = f\"{HOME}/yolov9/runs/train/exp/weights/best.onnx\"\n    shutil.copy(UPLOADED_ONNX, ONNX_WEIGHTS)\n    \n    print(f\"‚úÖ ONNX loaded to: {ONNX_WEIGHTS}\")\n    print(f\"üìä Size: {os.path.getsize(ONNX_WEIGHTS) / 1024 / 1024:.1f} MB\")\nelse:\n    print(f\"‚ùå ONNX not found at: {UPLOADED_ONNX}\")\n    print(\"\\nüí° Steps to fix:\")\n    print(\"   1. Click 'Add data' in right sidebar\")\n    print(\"   2. Select your dataset with best.onnx\")\n    print(\"   3. Update the path above with your dataset name\")\n    print(\"\\nAvailable datasets:\")\n    if os.path.exists('/kaggle/input'):\n        for dataset in os.listdir('/kaggle/input'):\n            print(f\"   - /kaggle/input/{dataset}\")\n    ONNX_WEIGHTS = None","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"IC7wJt-qnuz_","outputId":"71e637e9-e553-4622-8ab0-77d8455549b9","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:42.061527Z","iopub.execute_input":"2026-02-10T09:36:42.061883Z","iopub.status.idle":"2026-02-10T09:36:42.205352Z","shell.execute_reply.started":"2026-02-10T09:36:42.061850Z","shell.execute_reply":"2026-02-10T09:36:42.204396Z"}},"outputs":[{"name":"stdout","text":"üì• Loading best.onnx from Kaggle dataset...\n‚úÖ ONNX loaded to: /kaggle/working/yolov9/yolov9/runs/train/exp/weights/best.onnx\nüìä Size: 96.4 MB\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"if ONNX_WEIGHTS and os.path.exists(ONNX_WEIGHTS):\n    import torch\n    import numpy as np\n    import time\n    import onnxruntime as ort\n    import matplotlib.pyplot as plt\n\n    print(\"\\nüöÄ BENCHMARKING...\")\n\n    test_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n    num_runs = 100\n\n    # PyTorch\n    print(\"‚öôÔ∏è  PyTorch...\")\n    ckpt = torch.load(BEST_WEIGHTS, map_location='cpu', weights_only=False)\n    pytorch_model = ckpt['model'].float().eval().cuda()\n\n    with torch.no_grad():\n        for _ in range(10):\n            img = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().cuda() / 255.0\n            _ = pytorch_model(img)\n\n    pytorch_times = []\n    for i in range(num_runs):\n        img = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().cuda() / 255.0\n        torch.cuda.synchronize()\n        start = time.time()\n        with torch.no_grad():\n            _ = pytorch_model(img)\n        torch.cuda.synchronize()\n        pytorch_times.append((time.time() - start) * 1000)\n\n    pytorch_avg = np.mean(pytorch_times[10:])\n    pytorch_fps = 1000 / pytorch_avg\n\n    # ONNX\n    print(\"‚öôÔ∏è  ONNX...\")\n    ort_session = ort.InferenceSession(ONNX_WEIGHTS, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n    input_name = ort_session.get_inputs()[0].name\n    onnx_input = test_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n    onnx_input = np.expand_dims(onnx_input, axis=0)\n\n    for _ in range(10):\n        _ = ort_session.run(None, {input_name: onnx_input})\n\n    onnx_times = []\n    for i in range(num_runs):\n        start = time.time()\n        _ = ort_session.run(None, {input_name: onnx_input})\n        onnx_times.append((time.time() - start) * 1000)\n\n    onnx_avg = np.mean(onnx_times[10:])\n    onnx_fps = 1000 / onnx_avg\n    speedup = pytorch_avg / onnx_avg\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä FINAL RESULTS\")\n    print(\"=\"*60)\n    print(f\"PyTorch: {pytorch_avg:.1f}ms | {pytorch_fps:.1f} FPS\")\n    print(f\"ONNX:    {onnx_avg:.1f}ms | {onnx_fps:.1f} FPS\")\n    print(f\"Speedup: {speedup:.1f}x faster ‚ö°\")\n    print(\"=\"*60)\n\n    print(\"\\nüìù RESUME BULLET POINT:\")\n    print(f\"\"\"\nAutomatic Number Plate Recognition System | Python, YOLOv9, ByteTrack, ONNX\n\n- Trained YOLOv9 GELAN-C on custom dataset, integrated ByteTrack for vehicle tracking\n- Optimized model to ONNX achieving {speedup:.1f}x faster inference ({pytorch_avg:.0f}ms ‚Üí {onnx_avg:.0f}ms on T4 GPU)\n- Deployed system processing {onnx_fps:.0f}+ FPS with EasyOCR text extraction\n- Tracked training with TensorBoard across 25 epochs\n\"\"\")\nelse:\n    print(\"‚ùå Upload ONNX file first!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Hxm97iYxpzxw","outputId":"1c8059d6-97bc-43ed-9111-fce85d2cbaca","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:36:45.368883Z","iopub.execute_input":"2026-02-10T09:36:45.369199Z","iopub.status.idle":"2026-02-10T09:36:51.693899Z","shell.execute_reply.started":"2026-02-10T09:36:45.369172Z","shell.execute_reply":"2026-02-10T09:36:51.693139Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ BENCHMARKING...\n‚öôÔ∏è  PyTorch...\n‚öôÔ∏è  ONNX...\n\n============================================================\nüìä FINAL RESULTS\n============================================================\nPyTorch: 22.7ms | 44.0 FPS\nONNX:    27.0ms | 37.1 FPS\nSpeedup: 0.8x faster ‚ö°\n============================================================\n\nüìù RESUME BULLET POINT:\n\nAutomatic Number Plate Recognition System | Python, YOLOv9, ByteTrack, ONNX\n\n- Trained YOLOv9 GELAN-C on custom dataset, integrated ByteTrack for vehicle tracking\n- Optimized model to ONNX achieving 0.8x faster inference (23ms ‚Üí 27ms on T4 GPU)\n- Deployed system processing 37+ FPS with EasyOCR text extraction\n- Tracked training with TensorBoard across 25 epochs\n\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"import shutil\nimport os\n\n# REPLACE 'your-dataset-name' with actual dataset name\nUPLOADED_ONNX = '/kaggle/input/anpr-data-set/best.onnx'\n\nHOME = '/kaggle/working'\nos.makedirs(f\"{HOME}/yolov9/runs/train/exp2/weights\", exist_ok=True)\n\nONNX_WEIGHTS = f\"{HOME}/yolov9/runs/train/exp2/weights/best.onnx\"\n\nif os.path.exists(UPLOADED_ONNX):\n    shutil.copy(UPLOADED_ONNX, ONNX_WEIGHTS)\n    print(f\"‚úÖ ONNX copied to: {ONNX_WEIGHTS}\")\n    print(f\"üìä Size: {os.path.getsize(ONNX_WEIGHTS) / 1024 / 1024:.1f} MB\")\nelse:\n    print(f\"‚ùå Not found: {UPLOADED_ONNX}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:37:00.364444Z","iopub.execute_input":"2026-02-10T09:37:00.365186Z","iopub.status.idle":"2026-02-10T09:37:00.490583Z","shell.execute_reply.started":"2026-02-10T09:37:00.365156Z","shell.execute_reply":"2026-02-10T09:37:00.489842Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ONNX copied to: /kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\nüìä Size: 96.4 MB\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"!pip install onnxscript\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üöÄ ONNX OPTIMIZATION + MLOPS TRACKING\")\nprint(\"=\"*60)\n\nimport torch\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport onnxruntime as ort\nfrom pathlib import Path\nimport os\n\n# Setup paths\nHOME = '/kaggle/working'\nONNX_WEIGHTS = '/kaggle/working/yolov9/runs/train/exp2/weights/best.onnx'\nlatest_exp = f'{HOME}/yolov9/runs/train/exp2'\n\n# Setup TensorBoard\nlog_dir = f\"{HOME}/yolov9/runs/tensorboard/onnx_benchmark\"\nwriter = SummaryWriter(log_dir)\n\nprint(f\"‚úÖ TensorBoard logs: {log_dir}\")\n\n# Load training results\nresults_csv = f\"{latest_exp}/results.csv\"\n\nif os.path.exists(results_csv):\n    import pandas as pd\n    results_df = pd.read_csv(results_csv)\n    results_df.columns = results_df.columns.str.strip()\n\n    print(f\"‚úÖ Found training results: {len(results_df)} epochs\")\n\n    # Log to TensorBoard\n    for idx, row in results_df.iterrows():\n        if 'train/box_loss' in results_df.columns:\n            writer.add_scalar('Train/BoxLoss', row['train/box_loss'], idx)\n        if 'metrics/mAP50(B)' in results_df.columns:\n            writer.add_scalar('Val/mAP50', row['metrics/mAP50(B)'], idx)\n\n    final_metrics = results_df.iloc[-1]\n    print(f\"\\nüìä Final mAP@0.5: {final_metrics.get('metrics/mAP50(B)', 0):.4f}\")\n\n# USE YOUR ONNX FILE\nprint(\"\\nüì¶ Loading ONNX model from your path...\")\nonnx_path = ONNX_WEIGHTS\n\nif os.path.exists(onnx_path):\n    print(f\"‚úÖ ONNX model found: {onnx_path}\")\n    print(f\"üìä Size: {os.path.getsize(onnx_path) / 1024 / 1024:.1f} MB\")\nelse:\n    print(f\"‚ùå ONNX file not found at: {onnx_path}\")\n    print(\"\\nüí° Checking if file exists in Kaggle datasets...\")\n    \n    # Try to find and copy from dataset\n    import glob\n    found_onnx = glob.glob('/kaggle/input/**/best.onnx', recursive=True)\n    if found_onnx:\n        print(f\"‚úÖ Found ONNX in dataset: {found_onnx[0]}\")\n        import shutil\n        os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n        shutil.copy(found_onnx[0], onnx_path)\n        print(f\"‚úÖ Copied to: {onnx_path}\")\n    else:\n        onnx_path = None\n\nif onnx_path and os.path.exists(onnx_path):\n    writer.add_text('Model/ONNX_Export', f'Using ONNX: {onnx_path}')\n\n    # Prepare test\n    test_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n    num_runs = 100\n\n    # Get BEST_WEIGHTS path for PyTorch model\n    BEST_WEIGHTS = onnx_path.replace('.onnx', '.pt')\n    \n    if not os.path.exists(BEST_WEIGHTS):\n        # Try to find .pt file\n        pt_files = glob.glob(f'{latest_exp}/weights/*.pt')\n        if pt_files:\n            BEST_WEIGHTS = pt_files[0]\n            print(f\"‚úÖ Using PyTorch weights: {BEST_WEIGHTS}\")\n\n    # Benchmark PyTorch\n    if os.path.exists(BEST_WEIGHTS):\n        print(\"\\n‚ö° Benchmarking PyTorch...\")\n\n        # Load PyTorch model directly\n        ckpt = torch.load(BEST_WEIGHTS, map_location='cpu', weights_only=False)\n        pytorch_model = ckpt['model'].float().eval()\n        \n        # Move to GPU if available\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        pytorch_model = pytorch_model.to(device)\n        print(f\"   Using device: {device}\")\n\n        # Warmup\n        with torch.no_grad():\n            for _ in range(10):\n                img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n                _ = pytorch_model(img_tensor)\n\n        # Benchmark\n        pytorch_times = []\n        for i in range(num_runs):\n            img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n\n            if device == 'cuda':\n                torch.cuda.synchronize()\n            start = time.time()\n\n            with torch.no_grad():\n                _ = pytorch_model(img_tensor)\n\n            if device == 'cuda':\n                torch.cuda.synchronize()\n            pytorch_times.append((time.time() - start) * 1000)\n\n            if (i + 1) % 20 == 0:\n                print(f\"  Progress: {i+1}/{num_runs}\")\n\n        pytorch_mean = np.mean(pytorch_times[10:])\n        pytorch_fps = 1000 / pytorch_mean\n\n        print(f\"‚úÖ PyTorch: {pytorch_mean:.2f}ms | {pytorch_fps:.1f} FPS\")\n    else:\n        print(\"‚ö†Ô∏è  PyTorch weights not found, skipping PyTorch benchmark\")\n        pytorch_mean = 0\n        pytorch_fps = 0\n\n    # Benchmark ONNX\n    print(\"\\n‚ö° Benchmarking ONNX...\")\n\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n    ort_session = ort.InferenceSession(onnx_path, providers=providers)\n    input_name = ort_session.get_inputs()[0].name\n\n    print(f\"   Using provider: {ort_session.get_providers()[0]}\")\n\n    onnx_input = test_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n    onnx_input = np.expand_dims(onnx_input, axis=0)\n\n    # Warmup\n    for _ in range(10):\n        _ = ort_session.run(None, {input_name: onnx_input})\n\n    onnx_times = []\n    for i in range(num_runs):\n        start = time.time()\n        _ = ort_session.run(None, {input_name: onnx_input})\n        onnx_times.append((time.time() - start) * 1000)\n\n        if (i + 1) % 20 == 0:\n            print(f\"  Progress: {i+1}/{num_runs}\")\n\n    onnx_mean = np.mean(onnx_times[10:])\n    onnx_fps = 1000 / onnx_mean\n\n    print(f\"‚úÖ ONNX: {onnx_mean:.2f}ms | {onnx_fps:.1f} FPS\")\n\n    # Calculate speedup\n    if pytorch_mean > 0:\n        speedup = pytorch_mean / onnx_mean\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"üìä RESULTS\")\n        print(\"=\"*60)\n        print(f\"PyTorch: {pytorch_mean:.2f}ms | {pytorch_fps:.1f} FPS\")\n        print(f\"ONNX:    {onnx_mean:.2f}ms | {onnx_fps:.1f} FPS\")\n        print(f\"Speedup: {speedup:.2f}x faster ‚ö°\")\n        print(\"=\"*60)\n\n        # Log to TensorBoard\n        writer.add_scalar('Performance/PyTorch_ms', pytorch_mean, 0)\n        writer.add_scalar('Performance/ONNX_ms', onnx_mean, 0)\n        writer.add_scalar('Speedup/Multiplier', speedup, 0)\n\n        # Visualize\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n        axes[0].bar(['PyTorch', 'ONNX'], [pytorch_mean, onnx_mean], color=['#e74c3c', '#2ecc71'])\n        axes[0].set_ylabel('Inference Time (ms)')\n        axes[0].set_title('Speed Comparison')\n        axes[0].grid(axis='y', alpha=0.3)\n\n        axes[1].bar(['PyTorch', 'ONNX'], [pytorch_fps, onnx_fps], color=['#e74c3c', '#2ecc71'])\n        axes[1].set_ylabel('FPS')\n        axes[1].set_title('Frames Per Second')\n        axes[1].grid(axis='y', alpha=0.3)\n\n        plt.tight_layout()\n        plt.savefig('benchmark_comparison.png', dpi=150)\n        plt.show()\n\n        writer.add_figure('Benchmark/Comparison', fig, 0)\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(\"üìä ONNX ONLY RESULTS\")\n        print(\"=\"*60)\n        print(f\"ONNX: {onnx_mean:.2f}ms | {onnx_fps:.1f} FPS\")\n        print(\"=\"*60)\n        \n        writer.add_scalar('Performance/ONNX_ms', onnx_mean, 0)\n\n    writer.close()\n\n    print(\"\\n‚úÖ Benchmark complete!\")\n    print(f\"üìä TensorBoard logs saved to: {log_dir}\")\nelse:\n    print(\"\\n‚ùå Cannot proceed without ONNX file!\")\n    print(\"Please ensure best.onnx is uploaded to Kaggle dataset and added to your notebook.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":751},"id":"6uCX3I5CeY-V","outputId":"4bd007bd-6fa1-4316-bdca-7aa1acb35fa6","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:37:04.016655Z","iopub.execute_input":"2026-02-10T09:37:04.017017Z","iopub.status.idle":"2026-02-10T09:37:14.037208Z","shell.execute_reply.started":"2026-02-10T09:37:04.016989Z","shell.execute_reply":"2026-02-10T09:37:14.036379Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.6.1)\nRequirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\nRequirement already satisfied: onnx_ir<2,>=0.1.15 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.16)\nRequirement already satisfied: onnx>=1.17 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (26.0rc2)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\nRequirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17->onnxscript) (5.29.5)\n\n============================================================\nüöÄ ONNX OPTIMIZATION + MLOPS TRACKING\n============================================================\n‚úÖ TensorBoard logs: /kaggle/working/yolov9/runs/tensorboard/onnx_benchmark\n‚úÖ Found training results: 25 epochs\n\nüìä Final mAP@0.5: 0.0000\n\nüì¶ Loading ONNX model from your path...\n‚úÖ ONNX model found: /kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\nüìä Size: 96.4 MB\n\n‚ö° Benchmarking PyTorch...\n   Using device: cuda\n  Progress: 20/100\n  Progress: 40/100\n  Progress: 60/100\n  Progress: 80/100\n  Progress: 100/100\n‚úÖ PyTorch: 23.12ms | 43.3 FPS\n\n‚ö° Benchmarking ONNX...\n   Using provider: CUDAExecutionProvider\n  Progress: 20/100\n  Progress: 40/100\n  Progress: 60/100\n  Progress: 80/100\n  Progress: 100/100\n‚úÖ ONNX: 26.92ms | 37.2 FPS\n\n============================================================\nüìä RESULTS\n============================================================\nPyTorch: 23.12ms | 43.3 FPS\nONNX:    26.92ms | 37.2 FPS\nSpeedup: 0.86x faster ‚ö°\n============================================================\n\n‚úÖ Benchmark complete!\nüìä TensorBoard logs saved to: /kaggle/working/yolov9/runs/tensorboard/onnx_benchmark\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"üöÄ OPTIMIZED ONNX BENCHMARK\")\nprint(\"=\"*60)\n\nimport torch\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport onnxruntime as ort\nfrom pathlib import Path\nimport os\n\n# Configuration - KAGGLE PATHS\nHOME = \"/kaggle/working\"\nBEST_WEIGHTS = \"/kaggle/working/yolov9/runs/train/exp2/weights/best.pt\"\nonnx_path = \"/kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\"\n\nprint(\"\\nüìä Model Info:\")\nprint(f\"PyTorch: {BEST_WEIGHTS}\")\nprint(f\"ONNX: {onnx_path}\")\n\nif os.path.exists(onnx_path):\n    print(f\"Size: {os.path.getsize(onnx_path) / 1024 / 1024:.1f} MB\")\nelse:\n    print(\"‚ùå ONNX file not found!\")\n\n# Test configuration\ntest_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\nnum_warmup = 20\nnum_runs = 100\n\n# Check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nüñ•Ô∏è  Using device: {device}\")\n\n# ============================================================\n# 1. BASELINE PYTORCH (FP32)\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚ö° PYTORCH FP32 BASELINE\")\nprint(\"=\"*60)\n\nckpt = torch.load(BEST_WEIGHTS, map_location='cpu', weights_only=False)\npytorch_model = ckpt['model'].float().eval().to(device)\n\n# Warmup\nwith torch.no_grad():\n    for _ in range(num_warmup):\n        img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n        _ = pytorch_model(img_tensor)\n\n# Benchmark\nif device == 'cuda':\n    torch.cuda.synchronize()\npytorch_times = []\nfor i in range(num_runs):\n    img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n\n    if device == 'cuda':\n        torch.cuda.synchronize()\n    start = time.perf_counter()\n\n    with torch.no_grad():\n        _ = pytorch_model(img_tensor)\n\n    if device == 'cuda':\n        torch.cuda.synchronize()\n    pytorch_times.append((time.perf_counter() - start) * 1000)\n\npytorch_mean = np.mean(pytorch_times[10:])\npytorch_fps = 1000 / pytorch_mean\nprint(f\"‚úÖ PyTorch FP32: {pytorch_mean:.2f}ms | {pytorch_fps:.1f} FPS\")\n\n# ============================================================\n# 2. PYTORCH FP16 (HALF PRECISION) - Only if GPU available\n# ============================================================\nif device == 'cuda':\n    print(\"\\n\" + \"=\"*60)\n    print(\"‚ö° PYTORCH FP16 (HALF PRECISION)\")\n    print(\"=\"*60)\n\n    pytorch_model_fp16 = pytorch_model.half()\n\n    # Warmup\n    with torch.no_grad():\n        for _ in range(num_warmup):\n            img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).half().cuda() / 255.0\n            _ = pytorch_model_fp16(img_tensor)\n\n    # Benchmark\n    torch.cuda.synchronize()\n    pytorch_fp16_times = []\n    for i in range(num_runs):\n        img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0).half().cuda() / 255.0\n\n        torch.cuda.synchronize()\n        start = time.perf_counter()\n\n        with torch.no_grad():\n            _ = pytorch_model_fp16(img_tensor)\n\n        torch.cuda.synchronize()\n        pytorch_fp16_times.append((time.perf_counter() - start) * 1000)\n\n    pytorch_fp16_mean = np.mean(pytorch_fp16_times[10:])\n    pytorch_fp16_fps = 1000 / pytorch_fp16_mean\n    print(f\"‚úÖ PyTorch FP16: {pytorch_fp16_mean:.2f}ms | {pytorch_fp16_fps:.1f} FPS\")\n    print(f\"üìà Speedup vs FP32: {pytorch_mean/pytorch_fp16_mean:.2f}x\")\nelse:\n    pytorch_fp16_mean = None\n    pytorch_fp16_fps = None\n\n# ============================================================\n# 3. ONNX WITH OPTIMIZATION\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚ö° ONNX OPTIMIZED (FP32)\")\nprint(\"=\"*60)\n\n# Session options for optimization\nsess_options = ort.SessionOptions()\nsess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.intra_op_num_threads = 4\nsess_options.inter_op_num_threads = 4\nsess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n\n# Use CUDA with optimizations if available\nif device == 'cuda':\n    providers = [\n        ('CUDAExecutionProvider', {\n            'device_id': 0,\n            'arena_extend_strategy': 'kNextPowerOfTwo',\n            'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB\n            'cudnn_conv_algo_search': 'EXHAUSTIVE',\n            'do_copy_in_default_stream': True,\n        }),\n        'CPUExecutionProvider'\n    ]\nelse:\n    providers = ['CPUExecutionProvider']\n\nort_session = ort.InferenceSession(onnx_path, sess_options, providers=providers)\ninput_name = ort_session.get_inputs()[0].name\n\nprint(f\"üîç Using provider: {ort_session.get_providers()[0]}\")\n\n# Prepare input\nonnx_input = test_img.transpose(2, 0, 1).astype(np.float32) / 255.0\nonnx_input = np.expand_dims(onnx_input, axis=0)\n\n# Warmup\nfor _ in range(num_warmup):\n    _ = ort_session.run(None, {input_name: onnx_input})\n\n# Benchmark\nonnx_times = []\nfor i in range(num_runs):\n    start = time.perf_counter()\n    _ = ort_session.run(None, {input_name: onnx_input})\n    onnx_times.append((time.perf_counter() - start) * 1000)\n\nonnx_mean = np.mean(onnx_times[10:])\nonnx_fps = 1000 / onnx_mean\nprint(f\"‚úÖ ONNX Optimized: {onnx_mean:.2f}ms | {onnx_fps:.1f} FPS\")\nprint(f\"üìà Speedup vs PyTorch FP32: {pytorch_mean/onnx_mean:.2f}x\")\n\n# ============================================================\n# 4. ONNX FP16 (IF AVAILABLE)\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚ö° TRYING ONNX FP16 CONVERSION\")\nprint(\"=\"*60)\n\ntry:\n    !pip install -q onnxconverter-common\n    import onnx\n    from onnxconverter_common import float16\n\n    # Convert to FP16\n    onnx_fp16_path = \"/kaggle/working/yolov9/runs/train/exp2/weights/best_fp16.onnx\"\n\n    if not os.path.exists(onnx_fp16_path):\n        print(\"Converting ONNX to FP16...\")\n        model = onnx.load(onnx_path)\n        model_fp16 = float16.convert_float_to_float16(model)\n        onnx.save(model_fp16, onnx_fp16_path)\n        print(f\"‚úÖ FP16 model saved: {onnx_fp16_path}\")\n    else:\n        print(f\"‚úÖ FP16 model exists: {onnx_fp16_path}\")\n\n    print(f\"üìä FP16 Size: {os.path.getsize(onnx_fp16_path) / 1024 / 1024:.1f} MB\")\n\n    # Create FP16 session\n    ort_session_fp16 = ort.InferenceSession(onnx_fp16_path, sess_options, providers=providers)\n\n    # Warmup\n    for _ in range(num_warmup):\n        _ = ort_session_fp16.run(None, {input_name: onnx_input})\n\n    # Benchmark\n    onnx_fp16_times = []\n    for i in range(num_runs):\n        start = time.perf_counter()\n        _ = ort_session_fp16.run(None, {input_name: onnx_input})\n        onnx_fp16_times.append((time.perf_counter() - start) * 1000)\n\n    onnx_fp16_mean = np.mean(onnx_fp16_times[10:])\n    onnx_fp16_fps = 1000 / onnx_fp16_mean\n    print(f\"‚úÖ ONNX FP16: {onnx_fp16_mean:.2f}ms | {onnx_fp16_fps:.1f} FPS\")\n    print(f\"üìà Speedup vs PyTorch FP32: {pytorch_mean/onnx_fp16_mean:.2f}x\")\n\n    has_fp16 = True\nexcept Exception as e:\n    print(f\"‚ùå FP16 conversion failed: {e}\")\n    print(\"Installing onnxconverter-common might help...\")\n    has_fp16 = False\n    onnx_fp16_mean = None\n    onnx_fp16_fps = None\n\n# ============================================================\n# 5. TENSORRT (IF AVAILABLE)\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚ö° TRYING TENSORRT\")\nprint(\"=\"*60)\n\ntry:\n    providers_trt = [\n        ('TensorrtExecutionProvider', {\n            'device_id': 0,\n            'trt_max_workspace_size': 2147483648,  # 2GB\n            'trt_fp16_enable': True,\n            'trt_max_partition_iterations': 1000,\n            'trt_min_subgraph_size': 1,\n        }),\n        ('CUDAExecutionProvider', {\n            'device_id': 0,\n        }),\n        'CPUExecutionProvider'\n    ]\n\n    print(\"Building TensorRT engine (this may take a minute)...\")\n    ort_session_trt = ort.InferenceSession(onnx_path, sess_options, providers=providers_trt)\n\n    # Warmup\n    for _ in range(num_warmup):\n        _ = ort_session_trt.run(None, {input_name: onnx_input})\n\n    # Benchmark\n    trt_times = []\n    for i in range(num_runs):\n        start = time.perf_counter()\n        _ = ort_session_trt.run(None, {input_name: onnx_input})\n        trt_times.append((time.perf_counter() - start) * 1000)\n\n    trt_mean = np.mean(trt_times[10:])\n    trt_fps = 1000 / trt_mean\n    print(f\"‚úÖ TensorRT: {trt_mean:.2f}ms | {trt_fps:.1f} FPS\")\n    print(f\"üìà Speedup vs PyTorch FP32: {pytorch_mean/trt_mean:.2f}x\")\n\n    has_trt = True\nexcept Exception as e:\n    print(f\"‚ùå TensorRT not available: {e}\")\n    has_trt = False\n    trt_mean = None\n    trt_fps = None\n\n# ============================================================\n# FINAL COMPARISON\n# ============================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä FINAL BENCHMARK RESULTS\")\nprint(\"=\"*70)\n\nresults = [\n    (\"PyTorch FP32\", pytorch_mean, pytorch_fps, 1.0),\n]\n\nif pytorch_fp16_mean:\n    results.append((\"PyTorch FP16\", pytorch_fp16_mean, pytorch_fp16_fps, pytorch_mean/pytorch_fp16_mean))\n\nresults.append((\"ONNX Optimized FP32\", onnx_mean, onnx_fps, pytorch_mean/onnx_mean))\n\nif has_fp16:\n    results.append((\"ONNX FP16\", onnx_fp16_mean, onnx_fp16_fps, pytorch_mean/onnx_fp16_mean))\n\nif has_trt:\n    results.append((\"TensorRT FP16\", trt_mean, trt_fps, pytorch_mean/trt_mean))\n\n# Sort by inference time (fastest first)\nresults.sort(key=lambda x: x[1])\n\nprint(f\"{'Method':<25} {'Time (ms)':<12} {'FPS':<12} {'Speedup':<10}\")\nprint(\"-\" * 70)\nfor name, ms, fps, speedup in results:\n    print(f\"{name:<25} {ms:>8.2f}ms   {fps:>8.1f}     {speedup:>6.2f}x\")\nprint(\"=\"*70)\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nnames = [r[0] for r in results]\ntimes = [r[1] for r in results]\nfps_values = [r[2] for r in results]\n\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6'][:len(results)]\n\naxes[0].barh(names, times, color=colors)\naxes[0].set_xlabel('Inference Time (ms)')\naxes[0].set_title('Speed Comparison (Lower is Better)')\naxes[0].grid(axis='x', alpha=0.3)\naxes[0].invert_yaxis()\n\naxes[1].barh(names, fps_values, color=colors)\naxes[1].set_xlabel('FPS')\naxes[1].set_title('Throughput (Higher is Better)')\naxes[1].grid(axis='x', alpha=0.3)\naxes[1].invert_yaxis()\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/optimized_benchmark.png', dpi=150, bbox_inches='tight')\nprint(\"\\n‚úÖ Saved visualization: /kaggle/working/optimized_benchmark.png\")\nplt.show()\n\n# Best method\nbest = results[0]\nprint(f\"\\nüèÜ WINNER: {best[0]}\")\nprint(f\"   {best[1]:.2f}ms | {best[2]:.1f} FPS | {best[3]:.2f}x faster than baseline\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T09:37:18.322119Z","iopub.execute_input":"2026-02-10T09:37:18.322477Z","iopub.status.idle":"2026-02-10T09:37:35.875061Z","shell.execute_reply.started":"2026-02-10T09:37:18.322442Z","shell.execute_reply":"2026-02-10T09:37:35.874223Z"}},"outputs":[{"name":"stdout","text":"============================================================\nüöÄ OPTIMIZED ONNX BENCHMARK\n============================================================\n\nüìä Model Info:\nPyTorch: /kaggle/working/yolov9/runs/train/exp2/weights/best.pt\nONNX: /kaggle/working/yolov9/runs/train/exp2/weights/best.onnx\nSize: 96.4 MB\n\nüñ•Ô∏è  Using device: cuda\n\n============================================================\n‚ö° PYTORCH FP32 BASELINE\n============================================================\n‚úÖ PyTorch FP32: 23.27ms | 43.0 FPS\n\n============================================================\n‚ö° PYTORCH FP16 (HALF PRECISION)\n============================================================\n‚úÖ PyTorch FP16: 25.29ms | 39.5 FPS\nüìà Speedup vs FP32: 0.92x\n\n============================================================\n‚ö° ONNX OPTIMIZED (FP32)\n============================================================\nüîç Using provider: CUDAExecutionProvider\n‚úÖ ONNX Optimized: 26.92ms | 37.1 FPS\nüìà Speedup vs PyTorch FP32: 0.86x\n\n============================================================\n‚ö° TRYING ONNX FP16 CONVERSION\n============================================================\n‚úÖ FP16 model exists: /kaggle/working/yolov9/runs/train/exp2/weights/best_fp16.onnx\nüìä FP16 Size: 48.2 MB\n‚ùå FP16 conversion failed: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))\nInstalling onnxconverter-common might help...\n\n============================================================\n‚ö° TRYING TENSORRT\n============================================================\nBuilding TensorRT engine (this may take a minute)...\n*************** EP Error ***************\nEP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:539 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n when using [('TensorrtExecutionProvider', {'device_id': 0, 'trt_max_workspace_size': 2147483648, 'trt_fp16_enable': True, 'trt_max_partition_iterations': 1000, 'trt_min_subgraph_size': 1}), ('CUDAExecutionProvider', {'device_id': 0}), 'CPUExecutionProvider']\nFalling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n****************************************\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;31m2026-02-10 09:37:31.928829288 [E:onnxruntime:Default, provider_bridge_ort.cc:2336 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1957 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library /usr/local/lib/python3.12/dist-packages/onnxruntime/capi/libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n\u001b[m\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ TensorRT: 26.94ms | 37.1 FPS\nüìà Speedup vs PyTorch FP32: 0.86x\n\n======================================================================\nüìä FINAL BENCHMARK RESULTS\n======================================================================\nMethod                    Time (ms)    FPS          Speedup   \n----------------------------------------------------------------------\nPyTorch FP32                 23.27ms       43.0       1.00x\nPyTorch FP16                 25.29ms       39.5       0.92x\nONNX Optimized FP32          26.92ms       37.1       0.86x\nTensorRT FP16                26.94ms       37.1       0.86x\n======================================================================\n\n‚úÖ Saved visualization: /kaggle/working/optimized_benchmark.png\n\nüèÜ WINNER: PyTorch FP32\n   23.27ms | 43.0 FPS | 1.00x faster than baseline\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}